{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsEcnQNu8eih2H1lZvH1cx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2k8p-KIu5YFo","executionInfo":{"status":"ok","timestamp":1740438389279,"user_tz":480,"elapsed":4036,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"8249b227-1726-49c4-ba84-5965867a28aa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","source":["#  Step 1: Import Libraries\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score, classification_report\n","import optuna  # For hyperparameter tuning\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#  Step 2: Load Dataset\n","df = pd.read_csv(\"output_chunk_4.csv\")\n","df[\"text\"] = df[\"text\"].astype(str)  # Ensure text is string\n","\n","#  Step 3: Data Preprocessing\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download('punkt_tab') # Download the punkt_tab resource\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words(\"english\"))\n","\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove special characters\n","    tokens = word_tokenize(text)  # Tokenization\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatization & Stopword removal\n","    return \" \".join(tokens)\n","\n","df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n","\n","#  Step 4: Train-Test Split\n","X = df[\"clean_text\"]\n","y = df[\"category\"]\n","\n","# Split into Training (80%) and Temporary set (20%)\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","# Split Temporary set into Validation (18%) and Test (2%)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.1, stratify=y_temp, random_state=42)\n","\n","#  Step 5: Feature Extraction (TF-IDF)\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features if needed\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_val_tfidf = tfidf_vectorizer.transform(X_val)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","#  Step 6: Train and Evaluate Best Model\n","\n","# Train SVM Model\n","svm = LinearSVC(random_state=42)\n","svm.fit(X_train_tfidf, y_train)\n","\n","# Predict on Validation Set\n","y_val_pred = svm.predict(X_val_tfidf)\n","\n","# Compute Accuracy and Full Classification Report for Each Category\n","validation_accuracy = accuracy_score(y_val, y_val_pred)\n","classification_results = classification_report(y_val, y_val_pred, digits=3)\n","\n","print(f\"\\nValidation Accuracy: {validation_accuracy:.3f}\")\n","print(\"\\nValidation Classification Report:\")\n","print(classification_results)\n","\n","#  Step 7: Final Testing\n","\n","# Predict on Test Set\n","y_test_pred = svm.predict(X_test_tfidf)\n","\n","# Compute Accuracy and Full Classification Report for Each Category\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","test_results = classification_report(y_test, y_test_pred, digits=3)\n","\n","print(f\"\\nTest Accuracy: {test_accuracy:.3f}\")\n","print(\"\\nFinal Test Classification Report:\")\n","print(test_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7LM2uCJ6CWI","executionInfo":{"status":"ok","timestamp":1740438441045,"user_tz":480,"elapsed":45585,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"3e67632c-d6b5-487c-9413-82b2e7e2b9c1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Validation Accuracy: 0.810\n","\n","Validation Classification Report:\n","               precision    recall  f1-score   support\n","\n","         arts      0.795     0.861     0.827        72\n","        crime      0.810     0.889     0.848        72\n","     disaster      0.785     0.861     0.821        72\n","      economy      0.760     0.792     0.776        72\n","    education      0.931     0.931     0.931        72\n","environmental      0.942     0.903     0.922        72\n","       health      0.867     0.903     0.884        72\n","humanInterest      0.767     0.778     0.772        72\n","       labour      0.845     0.833     0.839        72\n","    lifestyle      0.792     0.847     0.819        72\n","        other      0.295     0.181     0.224        72\n","     politics      0.955     0.889     0.921        72\n","     religion      0.678     0.556     0.611        72\n","      science      0.835     0.917     0.874        72\n","       social      0.919     0.792     0.851        72\n","        sport      0.776     0.917     0.841        72\n","       unrest      0.770     0.792     0.781        72\n","      weather      0.872     0.944     0.907        72\n","\n","     accuracy                          0.810      1296\n","    macro avg      0.800     0.810     0.803      1296\n"," weighted avg      0.800     0.810     0.803      1296\n","\n","\n","Test Accuracy: 0.792\n","\n","Final Test Classification Report:\n","               precision    recall  f1-score   support\n","\n","         arts      0.600     0.750     0.667         8\n","        crime      0.875     0.875     0.875         8\n","     disaster      1.000     1.000     1.000         8\n","      economy      0.444     0.500     0.471         8\n","    education      0.778     0.875     0.824         8\n","environmental      1.000     1.000     1.000         8\n","       health      1.000     0.750     0.857         8\n","humanInterest      0.727     1.000     0.842         8\n","       labour      0.750     0.750     0.750         8\n","    lifestyle      0.714     0.625     0.667         8\n","        other      0.833     0.625     0.714         8\n","     politics      0.778     0.875     0.824         8\n","     religion      0.429     0.375     0.400         8\n","      science      0.778     0.875     0.824         8\n","       social      0.857     0.750     0.800         8\n","        sport      0.875     0.875     0.875         8\n","       unrest      1.000     0.750     0.857         8\n","      weather      1.000     1.000     1.000         8\n","\n","     accuracy                          0.792       144\n","    macro avg      0.802     0.792     0.791       144\n"," weighted avg      0.802     0.792     0.791       144\n","\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1BxJJpGwMoV-","outputId":"e0d87203-28df-4852-e38c-920428352e10","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740438445848,"user_tz":480,"elapsed":1067,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["arts: ['music', 'said', 'year', 'group', 'new', 'one', 'time', 'people', 'june', 'like']\n","crime: ['said', 'court', 'crime', 'war', 'fraud', 'year', 'corruption', 'state', 'international', 'also']\n","disaster: ['said', 'police', 'year', 'car', 'vehicle', 'road', 'driver', 'crash', 'man', 'june']\n","economy: ['market', 'year', 'said', 'price', 'rate', 'also', 'new', 'company', 'inflation', 'food']\n","education: ['university', 'student', 'degree', 'said', 'year', 'college', 'education', 'state', 'program', 'also']\n","environmental: ['waste', 'recycling', 'plastic', 'said', 'energy', 'market', 'year', 'management', 'new', 'company']\n","health: ['said', 'health', 'patient', 'care', 'covid', 'therapy', 'case', 'new', 'infection', 'people']\n","humanInterest: ['size', 'win', 'acc', 'ownership', 'statement', 'kb', 'act', 'beneficial', 'ceremony', 'said']\n","labour: ['health', 'work', 'security', 'job', 'said', 'employee', 'people', 'year', 'also', 'service']\n","lifestyle: ['home', 'life', 'said', 'time', 'one', 'year', 'day', 'new', 'get', 'people']\n","other: ['said', 'year', 'one', 'also', 'new', 'first', 'people', 'two', 'state', 'time']\n","politics: ['city', 'said', 'ballot', 'election', 'new', 'voter', 'state', 'vote', 'primary', 'would']\n","religion: ['said', 'year', 'people', 'one', 'time', 'church', 'new', 'also', 'school', 'day']\n","science: ['device', 'apple', 'power', 'new', 'market', 'iphone', 'also', 'software', 'chip', 'feature']\n","social: ['abortion', 'said', 'woman', 'state', 'year', 'law', 'service', 'life', 'court', 'right']\n","sport: ['game', 'run', 'first', 'inning', 'two', 'said', 'baseball', 'season', 'team', 'hit']\n","unrest: ['said', 'war', 'israel', 'year', 'palestinian', 'gaza', 'shooting', 'people', 'city', 'day']\n","weather: ['climate', 'change', 'said', 'year', 'forecast', 'temperature', 'global', 'high', 'new', 'world']\n"]}],"source":["from collections import Counter\n","\n","# Function to extract top N words per category\n","def get_top_words_by_category(df, category_column, text_column, top_n=10):\n","    category_word_freq = {}\n","\n","    for category in df[category_column].unique():\n","        text = \" \".join(df[df[category_column] == category][text_column])\n","        words = text.split()\n","        word_counts = Counter(words)\n","        top_words = [word for word, _ in word_counts.most_common(top_n)]\n","        category_word_freq[category] = top_words\n","\n","    return category_word_freq\n","\n","# Extract top words per category\n","top_words_by_category = get_top_words_by_category(df, \"category\", \"clean_text\")\n","\n","# Print the most frequent words per category\n","for category, words in top_words_by_category.items():\n","    print(f\"{category}: {words}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZdfrRp_FMoV-","executionInfo":{"status":"ok","timestamp":1740438449217,"user_tz":480,"elapsed":302,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}}},"outputs":[],"source":["import numpy as np\n","\n","# Function to create rule-based features\n","def create_rule_based_features(text, top_words_by_category):\n","    feature_vector = np.zeros(len(top_words_by_category))  # One feature per category\n","    words = set(text.split())  # Unique words in the text\n","\n","    for idx, (category, top_words) in enumerate(top_words_by_category.items()):\n","        if any(word in words for word in top_words):  # If a top word is present\n","            feature_vector[idx] = 1  # Mark feature as present\n","\n","    return feature_vector\n","\n","# Apply the function to create features\n","rule_based_features = np.array([create_rule_based_features(text, top_words_by_category) for text in df[\"clean_text\"]])\n"]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler\n","\n","# 1. Standardize Rule-Based Features (Important for SVM)\n","# Apply the function to create features for the training set\n","rule_based_features_train = np.array([create_rule_based_features(text, top_words_by_category) for text in X_train])\n","rule_based_features_val = np.array([create_rule_based_features(text, top_words_by_category) for text in X_val])\n","rule_based_features_test = np.array([create_rule_based_features(text, top_words_by_category) for text in X_test])\n","\n","scaler = StandardScaler()\n","rule_based_features_scaled_train = scaler.fit_transform(rule_based_features_train)\n","rule_based_features_scaled_val = scaler.transform(rule_based_features_val) # Use transform, not fit_transform for val and test\n","rule_based_features_scaled_test = scaler.transform(rule_based_features_test)\n","\n","# 2. Combine Features (TF-IDF + Rule-Based)\n","X_train_combined = np.hstack([X_train_tfidf.toarray(), rule_based_features_scaled_train])\n","X_val_combined = np.hstack([X_val_tfidf.toarray(), rule_based_features_scaled_val])\n","X_test_combined = np.hstack([X_test_tfidf.toarray(), rule_based_features_scaled_test])\n","\n","\n","# 3. Train SVM Model\n","svm_model = SVC(kernel='linear', random_state=42)  # You can try other kernels like 'rbf'\n","svm_model.fit(X_train_combined, y_train)\n","\n","# 4. Predict and Evaluate on Validation Set\n","y_val_pred = svm_model.predict(X_val_combined)\n","accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy: {accuracy:.3f}\")\n","print(classification_report(y_val, y_val_pred))\n","\n","# 5. Predict and Evaluate on Test Set\n","y_test_pred = svm_model.predict(X_test_combined)\n","accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Test Accuracy: {accuracy:.3f}\")\n","print(classification_report(y_test, y_test_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7070AvgZPZCm","executionInfo":{"status":"ok","timestamp":1740439393224,"user_tz":480,"elapsed":108538,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"112381c7-21ba-4544-89bb-cbdc1845b8df"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.757\n","               precision    recall  f1-score   support\n","\n","         arts       0.74      0.86      0.79        72\n","        crime       0.78      0.81      0.79        72\n","     disaster       0.71      0.83      0.77        72\n","      economy       0.70      0.76      0.73        72\n","    education       0.85      0.85      0.85        72\n","environmental       0.95      0.83      0.89        72\n","       health       0.89      0.81      0.85        72\n","humanInterest       0.71      0.69      0.70        72\n","       labour       0.83      0.82      0.83        72\n","    lifestyle       0.80      0.79      0.80        72\n","        other       0.25      0.29      0.27        72\n","     politics       0.87      0.82      0.84        72\n","     religion       0.65      0.56      0.60        72\n","      science       0.75      0.75      0.75        72\n","       social       0.91      0.74      0.82        72\n","        sport       0.72      0.83      0.77        72\n","       unrest       0.78      0.68      0.73        72\n","      weather       0.88      0.90      0.89        72\n","\n","     accuracy                           0.76      1296\n","    macro avg       0.77      0.76      0.76      1296\n"," weighted avg       0.77      0.76      0.76      1296\n","\n","Test Accuracy: 0.799\n","               precision    recall  f1-score   support\n","\n","         arts       0.64      0.88      0.74         8\n","        crime       1.00      0.88      0.93         8\n","     disaster       1.00      0.88      0.93         8\n","      economy       0.57      0.50      0.53         8\n","    education       0.78      0.88      0.82         8\n","environmental       0.89      1.00      0.94         8\n","       health       1.00      0.62      0.77         8\n","humanInterest       0.80      1.00      0.89         8\n","       labour       0.88      0.88      0.88         8\n","    lifestyle       0.62      0.62      0.62         8\n","        other       0.50      0.75      0.60         8\n","     politics       1.00      0.88      0.93         8\n","     religion       0.50      0.12      0.20         8\n","      science       0.78      0.88      0.82         8\n","       social       0.88      0.88      0.88         8\n","        sport       0.88      0.88      0.88         8\n","       unrest       0.78      0.88      0.82         8\n","      weather       1.00      1.00      1.00         8\n","\n","     accuracy                           0.80       144\n","    macro avg       0.80      0.80      0.79       144\n"," weighted avg       0.80      0.80      0.79       144\n","\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler\n","\n","# 1. Create and standardize rule-based features for train, val, and test sets:\n","\n","# a. Apply the create_rule_based_features function to each set:\n","rule_based_features_train = np.array([create_rule_based_features(text, top_words_by_category) for text in X_train])\n","rule_based_features_val = np.array([create_rule_based_features(text, top_words_by_category) for text in X_val])\n","rule_based_features_test = np.array([create_rule_based_features(text, top_words_by_category) for text in X_test])\n","\n","# b. Standardize the rule-based features:\n","scaler = StandardScaler()\n","rule_based_features_scaled_train = scaler.fit_transform(rule_based_features_train)\n","rule_based_features_scaled_val = scaler.transform(rule_based_features_val)  # Use transform, not fit_transform for val and test\n","rule_based_features_scaled_test = scaler.transform(rule_based_features_test)\n","\n","# 2. Combine Features (TF-IDF + Rule-Based):\n","X_train_combined = np.hstack([X_train_tfidf.toarray(), rule_based_features_scaled_train])\n","X_val_combined = np.hstack([X_val_tfidf.toarray(), rule_based_features_scaled_val])\n","X_test_combined = np.hstack([X_test_tfidf.toarray(), rule_based_features_scaled_test])\n","\n","# 3. Train SVM Model:\n","svm_model = SVC(kernel='linear', random_state=42)  # You can try other kernels like 'rbf'\n","svm_model.fit(X_train_combined, y_train)\n","\n","# 4. Predict and Evaluate on Validation Set:\n","y_val_pred = svm_model.predict(X_val_combined)\n","accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy: {accuracy:.3f}\")\n","print(classification_report(y_val, y_val_pred))\n","\n","# 5. Predict and Evaluate on Test Set:\n","y_test_pred = svm_model.predict(X_test_combined)\n","accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Test Accuracy: {accuracy:.3f}\")\n","print(classification_report(y_test, y_test_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pv31t0KSF7K","executionInfo":{"status":"ok","timestamp":1740439694534,"user_tz":480,"elapsed":113376,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"f37bddb2-284e-4d54-8e7a-9dc602b41fc0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.757\n","               precision    recall  f1-score   support\n","\n","         arts       0.74      0.86      0.79        72\n","        crime       0.78      0.81      0.79        72\n","     disaster       0.71      0.83      0.77        72\n","      economy       0.70      0.76      0.73        72\n","    education       0.85      0.85      0.85        72\n","environmental       0.95      0.83      0.89        72\n","       health       0.89      0.81      0.85        72\n","humanInterest       0.71      0.69      0.70        72\n","       labour       0.83      0.82      0.83        72\n","    lifestyle       0.80      0.79      0.80        72\n","        other       0.25      0.29      0.27        72\n","     politics       0.87      0.82      0.84        72\n","     religion       0.65      0.56      0.60        72\n","      science       0.75      0.75      0.75        72\n","       social       0.91      0.74      0.82        72\n","        sport       0.72      0.83      0.77        72\n","       unrest       0.78      0.68      0.73        72\n","      weather       0.88      0.90      0.89        72\n","\n","     accuracy                           0.76      1296\n","    macro avg       0.77      0.76      0.76      1296\n"," weighted avg       0.77      0.76      0.76      1296\n","\n","Test Accuracy: 0.799\n","               precision    recall  f1-score   support\n","\n","         arts       0.64      0.88      0.74         8\n","        crime       1.00      0.88      0.93         8\n","     disaster       1.00      0.88      0.93         8\n","      economy       0.57      0.50      0.53         8\n","    education       0.78      0.88      0.82         8\n","environmental       0.89      1.00      0.94         8\n","       health       1.00      0.62      0.77         8\n","humanInterest       0.80      1.00      0.89         8\n","       labour       0.88      0.88      0.88         8\n","    lifestyle       0.62      0.62      0.62         8\n","        other       0.50      0.75      0.60         8\n","     politics       1.00      0.88      0.93         8\n","     religion       0.50      0.12      0.20         8\n","      science       0.78      0.88      0.82         8\n","       social       0.88      0.88      0.88         8\n","        sport       0.88      0.88      0.88         8\n","       unrest       0.78      0.88      0.82         8\n","      weather       1.00      1.00      1.00         8\n","\n","     accuracy                           0.80       144\n","    macro avg       0.80      0.80      0.79       144\n"," weighted avg       0.80      0.80      0.79       144\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import torch\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","\n","# Download NLTK dependencies\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Load dataset\n","df = pd.read_csv(\"output_chunk_4.csv\")  # Update path\n","\n","# Split data\n","X = df[\"text\"]\n","y = df[\"category\"]\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.1, stratify=y_temp, random_state=42)\n","\n","# -----------------------------------------------\n","# Function to preprocess text\n","# -----------------------------------------------\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    return text\n","\n","X_train = X_train.apply(preprocess_text)\n","X_val = X_val.apply(preprocess_text)\n","X_test = X_test.apply(preprocess_text)\n","\n","# -----------------------------------------------\n","# 1. TF-IDF Embeddings\n","# -----------------------------------------------\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_val_tfidf = tfidf_vectorizer.transform(X_val)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# -----------------------------------------------\n","# 2. RoBERTa Embeddings\n","# -----------------------------------------------\n","roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","roberta_model = AutoModel.from_pretrained(\"roberta-base\")\n","\n","def get_roberta_embedding(text):\n","    inputs = roberta_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = roberta_model(**inputs)\n","    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n","\n","X_train_roberta = np.array([get_roberta_embedding(text) for text in tqdm(X_train.tolist())])\n","X_val_roberta = np.array([get_roberta_embedding(text) for text in tqdm(X_val.tolist())])\n","X_test_roberta = np.array([get_roberta_embedding(text) for text in tqdm(X_test.tolist())])\n","\n","# -----------------------------------------------\n","# 3. DistilBERT Embeddings\n","# -----------------------------------------------\n","distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","distilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n","\n","def get_distilbert_embedding(text):\n","    inputs = distilbert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = distilbert_model(**inputs)\n","    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n","\n","X_train_distilbert = np.array([get_distilbert_embedding(text) for text in tqdm(X_train.tolist())])\n","X_val_distilbert = np.array([get_distilbert_embedding(text) for text in tqdm(X_val.tolist())])\n","X_test_distilbert = np.array([get_distilbert_embedding(text) for text in tqdm(X_test.tolist())])\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"UFqaEv2Z6_aO","outputId":"0674a50e-c6cc-4805-8717-1e9730f969c1","executionInfo":{"status":"error","timestamp":1740384414836,"user_tz":480,"elapsed":47813,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'output_chunk_4.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ec1e66a3b060>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_chunk_4.csv\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_chunk_4.csv'"]}]},{"cell_type":"code","source":["# -----------------------------------------------\n","# Stacked Classifier (SVM + XGBoost + Logistic Regression)\n","# -----------------------------------------------\n","base_models = [\n","    (\"svm\", SVC(probability=True, random_state=42)),\n","    (\"xgb\", XGBClassifier(n_estimators=200, max_depth=10, learning_rate=0.1, random_state=42)),\n","    (\"lr\", LogisticRegression(max_iter=500, random_state=42))\n","]\n","\n","# Final Stacking Model\n","stacked_model = StackingClassifier(estimators=base_models, final_estimator=XGBClassifier())\n","\n","# -----------------------------------------------\n","# GridSearchCV for Hyperparameter Optimization\n","# -----------------------------------------------\n","param_grid = {\n","    \"final_estimator__n_estimators\": [100, 200, 300],\n","    \"final_estimator__max_depth\": [6, 10, 14],\n","    \"final_estimator__learning_rate\": [0.01, 0.05, 0.1]\n","}\n","\n","def train_evaluate_model(X_train_emb, X_val_emb, name):\n","    print(f\"\\nTraining Stacked Model with {name} embeddings...\")\n","\n","    grid_search = GridSearchCV(stacked_model, param_grid, cv=3, scoring=\"accuracy\", verbose=2, n_jobs=-1)\n","    grid_search.fit(X_train_emb, y_train)\n","\n","    best_model = grid_search.best_estimator_\n","\n","    y_val_pred = best_model.predict(X_val_emb)\n","    val_accuracy = accuracy_score(y_val, y_val_pred)\n","\n","    print(f\"\\nBest Parameters for {name}: {grid_search.best_params_}\")\n","    print(f\"Validation Accuracy ({name}): {val_accuracy:.3f}\")\n","    print(\"\\nValidation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n","\n","# Train and Evaluate for each embedding type\n","train_evaluate_model(X_train_tfidf, X_val_tfidf, \"TF-IDF\")\n","train_evaluate_model(X_train_roberta, X_val_roberta, \"RoBERTa\")\n","train_evaluate_model(X_train_distilbert, X_val_distilbert, \"DistilBERT\")"],"metadata":{"id":"qJj3k24y_Z0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------\n","# Final Evaluation on Test Set\n","# -----------------------------------------------\n","def test_final_model(X_train_emb, X_val_emb, X_test_emb, name):\n","    print(f\"\\nFinal Testing with {name} embeddings...\")\n","\n","    stacked_model.fit(X_train_emb, y_train)\n","    y_test_pred = stacked_model.predict(X_test_emb)\n","\n","    test_accuracy = accuracy_score(y_test, y_test_pred)\n","    print(f\"\\nFinal Test Accuracy ({name}): {test_accuracy:.3f}\")\n","    print(\"\\nFinal Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n","\n","# Test best model on test set for all embeddings\n","test_final_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, \"TF-IDF\")\n","test_final_model(X_train_roberta, X_val_roberta, X_test_roberta, \"RoBERTa\")\n","test_final_model(X_train_distilbert, X_val_distilbert, X_test_distilbert, \"DistilBERT\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"j681q1BUwbqE","executionInfo":{"status":"error","timestamp":1740381540541,"user_tz":480,"elapsed":413,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"2ef1dcc2-da22-4b04-9240-3fc5d56c1345"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_train_tfidf' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-274d37770947>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Test best model on test set for all embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_final_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TF-IDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtest_final_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_roberta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_roberta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_roberta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RoBERTa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtest_final_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_distilbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_distilbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_distilbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DistilBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","\n","# Download NLTK dependencies\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Load dataset\n","df = pd.read_csv(\"output_chunk_4.csv\")  # Update path\n","\n","# Split data\n","X = df[\"text\"]\n","y = df[\"category\"]\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.1, stratify=y_temp, random_state=42)\n","\n","print(f\"Dataset Loaded. Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bk6QlEZllX51","executionInfo":{"status":"ok","timestamp":1740427983104,"user_tz":480,"elapsed":18861,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"447cdbf0-a5e0-46f8-eefd-5a46b6775bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Dataset Loaded. Train: 5760, Val: 1296, Test: 144\n"]}]},{"cell_type":"code","source":["def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n","    return text\n","\n","X_train = X_train.apply(preprocess_text)\n","X_val = X_val.apply(preprocess_text)\n","X_test = X_test.apply(preprocess_text)\n","\n","print(\"Text Preprocessing Complete \")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tknt4d-flz_F","executionInfo":{"status":"ok","timestamp":1740428001355,"user_tz":480,"elapsed":190,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"0865a019-e674-4c6b-c9d8-305e7a15c61f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Text Preprocessing Complete \n"]}]},{"cell_type":"code","source":["tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_val_tfidf = tfidf_vectorizer.transform(X_val)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","print(f\"TF-IDF Embeddings Shape: {X_train_tfidf.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hk4u4HBul-MP","executionInfo":{"status":"ok","timestamp":1740428041091,"user_tz":480,"elapsed":27195,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"70c1dde1-05bd-4e86-f20c-f1b6162e8c56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Embeddings Shape: (5760, 5000)\n"]}]},{"cell_type":"code","source":["# save TF-IDF Embedding sepratelsy to use later\n","import joblib\n","\n","joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.joblib\")\n","\n","# show path of saved file\n","print(f\"TF-IDF Vectorizer saved to: tfidf_vectorizer.joblib\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztL5P4JOm1K_","executionInfo":{"status":"ok","timestamp":1740445066633,"user_tz":480,"elapsed":464,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"0a79b95a-db33-40a5-a3e7-7e4ad2cf26f0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Vectorizer saved to: tfidf_vectorizer.joblib\n"]}]},{"cell_type":"code","source":["pip install AutoTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iw64FLhhnoq_","executionInfo":{"status":"ok","timestamp":1740445238275,"user_tz":480,"elapsed":2825,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"e60153cb-bd30-4e6f-8b07-b23e4e2830f3"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement AutoTokenizer (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for AutoTokenizer\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["\n","import AutoTokenizer\n","\n","\n","roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","roberta_model = AutoModel.from_pretrained(\"roberta-base\")\n","\n","def batch_roberta_embeddings(text_list, batch_size=32):\n","    all_embeddings = []\n","    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Extracting RoBERTa Embeddings\"):\n","        batch_texts = text_list[i:i+batch_size]\n","        inputs = roberta_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        with torch.no_grad():\n","            outputs = roberta_model(**inputs)\n","        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","        all_embeddings.append(batch_embeddings)\n","    return np.vstack(all_embeddings)\n","\n","X_train_roberta = batch_roberta_embeddings(X_train.tolist())\n","X_val_roberta = batch_roberta_embeddings(X_val.tolist())\n","X_test_roberta = batch_roberta_embeddings(X_test.tolist())\n","\n","print(f\"RoBERTa Embeddings Shape: {X_train_roberta.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"dbfNaYZmmCZu","executionInfo":{"status":"error","timestamp":1740445255076,"user_tz":480,"elapsed":37,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"add45743-8b72-4b09-c37a-4a750cbe4daf"},"execution_count":21,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'AutoTokenizer'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-02c235bbd270>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import Autotokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mroberta_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roberta-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'AutoTokenizer'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","distilbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n","\n","def batch_distilbert_embeddings(text_list, batch_size=32):\n","    all_embeddings = []\n","    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Extracting DistilBERT Embeddings\"):\n","        batch_texts = text_list[i:i+batch_size]\n","        inputs = distilbert_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        with torch.no_grad():\n","            outputs = distilbert_model(**inputs)\n","        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","        all_embeddings.append(batch_embeddings)\n","    return np.vstack(all_embeddings)\n","\n","X_train_distilbert = batch_distilbert_embeddings(X_train.tolist())\n","X_val_distilbert = batch_distilbert_embeddings(X_val.tolist())\n","X_test_distilbert = batch_distilbert_embeddings(X_test.tolist())\n","\n","print(f\"DistilBERT Embeddings Shape: {X_train_distilbert.shape}\")\n"],"metadata":{"id":"4BxfX7hbmFdg","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1740445271167,"user_tz":480,"elapsed":53,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"ea918798-cb7b-4725-faeb-3194c3985e74"},"execution_count":22,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'AutoTokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-5bf5ccf8a7f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistilbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdistilbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_distilbert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"]}]},{"cell_type":"code","source":["svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n","svm_model.fit(X_train_tfidf, y_train)\n","\n","y_val_pred = svm_model.predict(X_val_tfidf)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","\n","print(f\"SVM Validation Accuracy: {val_accuracy:.3f}\")\n","print(\"\\nValidation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKS0MhSSmIvX","executionInfo":{"status":"ok","timestamp":1740428932057,"user_tz":480,"elapsed":263014,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"1218ffa0-4109-4995-998c-054335e849d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Validation Accuracy: 0.762\n","\n","Validation Classification Report:\n","                precision    recall  f1-score   support\n","\n","         arts       0.74      0.88      0.80        72\n","        crime       0.82      0.75      0.78        72\n","     disaster       0.76      0.81      0.78        72\n","      economy       0.70      0.74      0.72        72\n","    education       0.91      0.89      0.90        72\n","environmental       0.94      0.82      0.87        72\n","       health       0.87      0.82      0.84        72\n","humanInterest       0.73      0.67      0.70        72\n","       labour       0.85      0.83      0.84        72\n","    lifestyle       0.72      0.85      0.78        72\n","        other       0.25      0.31      0.28        72\n","     politics       0.92      0.83      0.88        72\n","     religion       0.58      0.56      0.57        72\n","      science       0.80      0.82      0.81        72\n","       social       0.92      0.67      0.77        72\n","        sport       0.75      0.88      0.81        72\n","       unrest       0.79      0.69      0.74        72\n","      weather       0.88      0.92      0.90        72\n","\n","     accuracy                           0.76      1296\n","    macro avg       0.77      0.76      0.76      1296\n"," weighted avg       0.77      0.76      0.76      1296\n","\n"]}]},{"cell_type":"code","source":["base_models = [\n","    (\"svm\", SVC(kernel=\"linear\", probability=True, random_state=42)),\n","    (\"xgb\", XGBClassifier(n_estimators=200, max_depth=10, learning_rate=0.1, random_state=42)),\n","    (\"lr\", LogisticRegression(max_iter=500, random_state=42))\n","]\n","\n","stacked_model = StackingClassifier(estimators=base_models, final_estimator=SVC(kernel=\"linear\", probability=True))\n","\n","stacked_model.fit(X_train_tfidf, y_train)\n","\n","y_val_pred = stacked_model.predict(X_val_tfidf)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","\n","print(f\"Stacked Model Validation Accuracy: {val_accuracy:.3f}\")\n","print(\"\\nValidation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"WpyUdjzMmLhd","executionInfo":{"status":"error","timestamp":1740434181202,"user_tz":480,"elapsed":3686244,"user":{"displayName":"Amrita Ajay Sagar","userId":"03428876864640888360"}},"outputId":"6e283b2a-e4ed-4a88-cdc6-e81b3f30149f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-aa276205e29c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstacked_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstacked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_val_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     @available_if(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             predictions = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 delayed(cross_val_predict)(\n\u001b[1;32m    256\u001b[0m                     \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m     predictions = parallel(\n\u001b[0m\u001b[1;32m   1248\u001b[0m         delayed(_fit_and_predict)(\n\u001b[1;32m   1249\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1597\u001b[0m             )\n\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1600\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["y_test_pred = stacked_model.predict(X_test_tfidf)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(f\"Final Test Accuracy: {test_accuracy:.3f}\")\n","print(\"\\nFinal Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"],"metadata":{"id":"DLEE4Y3rmN9O"},"execution_count":null,"outputs":[]}]}